{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbd018f",
   "metadata": {},
   "source": [
    "# Impact of Bias and Variance\n",
    "\n",
    "## Rewriting prediction error\n",
    "Consider the variation in our prediction error, $\\boldsymbol{e}$. To do so, let's rewrite it:\n",
    "$$e \\equiv y-\\boldsymbol{b}'\\boldsymbol{x}$$\n",
    "$$= y - \\boldsymbol{b}'x + \\underbrace{\\boldsymbol{\\beta}'x - \\boldsymbol{\\beta}'x + E[\\boldsymbol{b}]'x - E[\\boldsymbol{b}]'x}_{\\text{add and subtract these for convenience}}$$\n",
    "\n",
    "$$= (y-\\boldsymbol{\\beta}'x) - (E[\\boldsymbol{b}] - \\boldsymbol{\\beta})'x - (\\boldsymbol{b} - E[\\boldsymbol{b}])'x$$\n",
    "\n",
    "Recall that\n",
    "$$\\epsilon \\equiv y - \\boldsymbol{\\beta}'x$$\n",
    "\n",
    "Let $\\theta$ denote the bias of the estimator, (noting this is a constant, not a random variable,):\n",
    "$$\\boldsymbol{\\theta} \\equiv E[\\boldsymbol{b}] - \\boldsymbol{\\beta}$$\n",
    "\n",
    "Then\n",
    "$$e = \\epsilon - \\boldsymbol{\\theta}'\\boldsymbol{x} - (\\boldsymbol{b}-E[\\boldsymbol{b}])'\\boldsymbol{x}$$\n",
    "\n",
    "which can be interpreted as saying that the **prediction error** equals\n",
    "- noise\n",
    "- estimator bias\n",
    "- estimator variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab8d73",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "Now let's use this to consider the variance of the estimated errors. \n",
    "\n",
    "Let $E_x\\left[\\cdot\\right]$ denote the expectation conditional on $x$: $E\\left[\\cdot | \\boldsymbol{X}\\right]$. \n",
    "\n",
    "$$E_x\\left[e^2\\right] = E_x\\left[\\left(\\epsilon - \\boldsymbol{\\theta}'\\boldsymbol{x} - (\\boldsymbol{b}-E_x[\\boldsymbol{b}])'\\boldsymbol{x}\\right)^2\\right]$$\n",
    "\n",
    "$$= E_x\\left[\\epsilon^2\\right] + E_x\\left[(\\boldsymbol{\\theta}'\\boldsymbol{x})^2\\right] + E_x\\left[\\left((\\boldsymbol{b}-E_x[\\boldsymbol{b}]'\\boldsymbol{x})\\right)^2\\right] - 2E_x\\left[\\boldsymbol{\\theta}'\\boldsymbol{x}\\epsilon\\right]  - 2E_x\\left[(\\boldsymbol{b}-E_x[\\boldsymbol{b}])'\\boldsymbol{x}\\epsilon\\right] + 2E_x\\left[\\boldsymbol{\\theta}'\\boldsymbol{x}\\boldsymbol{x}'(\\boldsymbol{b}-E_x[\\boldsymbol{b}])\\right]$$\n",
    "\n",
    "$$= E_x\\left[\\epsilon^2\\right] + E_x\\left[(\\boldsymbol{\\theta}'\\boldsymbol{x})^2\\right] + E_x\\left[\\left((\\boldsymbol{b}-E_x[\\boldsymbol{b}])'\\boldsymbol{x}\\right)^2\\right] - 2\\boldsymbol{\\theta}'\\boldsymbol{x}\\underbrace{E_x\\left[\\epsilon\\right]}_{0}  - 2\\boldsymbol{x}'\\underbrace{E_x\\left[\\boldsymbol{b}\\epsilon\\right]}_{\\text{?}} + 2\\boldsymbol{x}'E_x[\\boldsymbol{b}]\\underbrace{E_x\\left[\\epsilon\\right]}_{0} + 2\\boldsymbol{\\theta}'\\boldsymbol{x}\\boldsymbol{x}'\\underbrace{E_x\\left[(\\boldsymbol{b}-E_x[\\boldsymbol{b}])\\right]}_{0}$$\n",
    "\n",
    "- Two of the terms zero out based on the exogeneity assumption: $E_x[\\epsilon]=0$.\n",
    "- The final term zeros out due to iterated expectations.\n",
    "\n",
    "\n",
    "The penultimate term, (marked with ?,) is more complicated.  \n",
    "- Recall that for any sample, the OLS estimator can be written as $\\boldsymbol{b} = \\boldsymbol{(X'X)^{-1}X'Y} = \\boldsymbol{\\beta} + \\boldsymbol{(X'X)^{-1}X'}\\epsilon$ \n",
    "- Clearly then, the estimator $\\boldsymbol{b}$ will be correlated to the in-sample residuals, and the term in question is not zero in-sample.\n",
    "$E_x\\left[\\boldsymbol{b}^{IS}\\epsilon^{IS}\\right]$\n",
    "- However, suppose we are considering the variance-bias tradeoff out-of-sample (OOS)? Under the mild assumption that IS $\\epsilon$ is uncorrelated with the OOS $\\epsilon$, we have \n",
    "$E_x\\left[\\boldsymbol{b}^{IS}\\epsilon^{OOS}\\right]=0$\n",
    "\n",
    "Thus, \n",
    "$$E_x\\left[e^2\\right] = E_x\\left[\\epsilon^2\\right] + E_x\\left[(\\boldsymbol{\\theta}'\\boldsymbol{x})^2\\right] + E_x\\left[\\left((\\boldsymbol{b}-E_x[\\boldsymbol{b}])'\\boldsymbol{x}\\right)^2\\right]$$\n",
    "\n",
    "$$\\underbrace{E_x\\left[e^2\\right]}_{\\text{prediction error variance}} = \\underbrace{\\sigma^2_\\epsilon}_{\\text{noise}} + \\underbrace{\\boldsymbol{x}'\\boldsymbol{\\theta}\\boldsymbol{\\theta}'\\boldsymbol{x}}_{\\text{bias squared}} + \\underbrace{\\boldsymbol{x}'\\boldsymbol{\\Sigma_{b}}\\boldsymbol{x}}_{\\text{variance of estimator}}$$\n",
    "where $\\Sigma_b$ is the covariance matrix of the OLS estimator.\n",
    "\n",
    "### Generalization for non-spherical errors\n",
    "If we want to allow for a richer covariance structure of the $n$ prediction errors, (suppose we have heteroskedasticity or serial correlation,) we can write the $n\\times 1$ vector, $\\boldsymbol{e}$ and the $n\\times k$ matrix $\\boldsymbol{X}$. Then derive similarly to above:\n",
    "$$\\underbrace{\\boldsymbol{\\Sigma_e}}_{\\text{prediction error variance}} = \\underbrace{\\boldsymbol{\\Sigma_\\epsilon}}_{\\text{noise}} + \\underbrace{\\boldsymbol{X}\\boldsymbol{\\theta}\\boldsymbol{\\theta}'\\boldsymbol{X'}}_{\\text{bias squared}} + \\underbrace{\\boldsymbol{X}\\boldsymbol{\\Sigma_{b}}\\boldsymbol{X}'}_{\\text{variance of estimator}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6fb7a",
   "metadata": {},
   "source": [
    "## Gauss-Markov Theorem for In-Sample OLS\n",
    "\n",
    "If the assumptions of the Gauss-Markov theorem are satisfied,\n",
    "- $\\boldsymbol{\\theta} = \\boldsymbol{0}$\n",
    "- $\\boldsymbol{\\Sigma}_\\epsilon = \\boldsymbol{\\mathcal{I}}_n\\sigma^2_\\epsilon$\n",
    "- $\\boldsymbol{\\Sigma_b} = (\\boldsymbol{X}'\\boldsymbol{X})^{-1}\\sigma^2_\\epsilon$\n",
    "- Suppose that we are considering the bias-variance tradeoff *in-sample*. Then the term above with the ? is non-zero.\n",
    "$$2\\boldsymbol{x}'E_x\\left[\\boldsymbol{b}\\epsilon\\right] = 2\\boldsymbol{X(X'X)^{-1}X'}\\sigma^2_\\epsilon$$\n",
    "\n",
    "Thus,\n",
    "$$\\underbrace{\\boldsymbol{\\Sigma_e}}_{\\text{prediction error variance}} = \\left(\\boldsymbol{\\mathcal{I}}_n - \\boldsymbol{X(X'X)^{-1}X'}\\right)\\sigma^2_\\epsilon $$\n",
    "Notably, the bias is zero, and the variance of the estimator is minimized subject to the bias being zero.\n",
    "\n",
    "$$\\underbrace{\\boldsymbol{\\Sigma_e}}_{\\text{prediction error variance}} = \\left(\\boldsymbol{\\mathcal{I}}_n - \\boldsymbol{X(X'X)^{-1}X'}\\right)\\sigma^2_\\epsilon $$\n",
    "\n",
    "This could have been easily derived up front by noting that\n",
    "$$\\boldsymbol{e} = \\left(\\boldsymbol{\\mathcal{I}}_n - \\boldsymbol{X(X'X)^{-1}X'}\\right)\\boldsymbol{y}$$\n",
    "$$ = \\boldsymbol{My}$$\n",
    "$$ = \\boldsymbol{M(X\\beta + \\epsilon)}$$\n",
    "where $\\boldsymbol{M}$ is the idempotent _annihalator_ matrix.\n",
    "\n",
    "## Biased but lower variance\n",
    "It is possible that a different estimator estimator has bias yet has smaller estimator variance, and that this tradeoff is worth it, in terms of reducing $E_x\\left[e^2\\right]$.\n",
    "\n",
    "In particular, this means that the total prediction error variance may be smaller for a linear model that omits some of the (true) regressors, thus taking on bias but reducing variance.\n",
    "\n",
    "For the technical conditions for which omitting variables would improve prediction error variance, see \n",
    "* The Use of Simplified or Misspecified Models: Linear Case by Shaohua Wu, T.J. Harris, and K.B. McAuley (2007)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
